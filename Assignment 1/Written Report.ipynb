{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Written Report Section:\n",
    "\n",
    "\n",
    "## 25.\n",
    "The Dataset has 4898 rows with 13 features. After getting rid of duplicate values the dataset is now 3938. The target of the dataset is to be able to predict the quality. The quality column is a value from 0 – 10, this column is converted into a new column which takes those values from quality and converts them to a 0 or a 1 (0 = quality <= 5, 1 = quality >5). The new quality (0,1) is the target of the machine learning algorithm. In order to do that we would have to have four different variables, an X test and X train, and a Y test and a Y train. The Y test and Y train are the labels or the “targets” for the machine learning. The X test and train, are the data which will be used to attempt to predict these targets.  We will be using the X train and Y train values in order to predict the quality of the X test values. The X test will then be compared to the actual values (Y test) and will be then evaluated on whether or not the Machine learning algorithm was successful or not. I dropped all the features that were not within the top 6 when correlated to the binary “quality” variable we created. I did this because their values were so close to 0 it felt redundant and almost negligible with or without them. The Features that I dropped were residual sugar, fixed acidity, pH, sulphates, free sulfur dioxide, and citric acid. Each of these had a correlation of less than 0.1. The only feature which I could’ve potentially left on was the residual sugar with a correlation of 0.09. I didn't drop any features because of redundency mainly because no correlation value was exactly the same. Standardizing the values in the dataset will be used because the F1 score and the Accuracy was more accuracte (Table 2. Table 3.). I've incldued both the orginal descriptive statistics of the dataset before dropping features and everything and after. \n",
    "\n",
    "## 26\n",
    "\n",
    "The run time complexity of the model is that, with the intrudction of folds, the time in which it spends calculating this proccess is increased from regularly running the model (20s) to now (~2 minutes). Increasing the values of K effects the number of distances that will be considered in predicting the outcome. With each increase in K, to a specific value of K, will cause the F1 rate to become better. Increasing K, will provide worse and worse outcomes based upon when in the distance metric is to far away and the number of those wrong distances start to out weigh the correct distances. It would appear that increasing K value doesn't mean that they're getting a better value from it. In the tables below, it shows that K = 5 is the most optimal metric to use, and with the increase in K it infact decreased it had a lower F1, as well as a lower accuracy. I decided against using distance weighting because the results from the initial testing showed that the benifits of weighting didn't cause it to be better than non weighted. \n",
    "\n",
    "## 27\n",
    "\n",
    "|Recall|Euclidean Weighted|Manhattan Weighted|Euclidean|Manhattan|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|**K=1**| 0.7355| 0.7456 | 0.7355| 0.7456 | \n",
    "|**K=5**| 0.7783| 0.7859| 0.8136| **0.8161** | \n",
    "|**K=9**| 0.7884| 0.796 | 0.8086| 0.8086 |  \n",
    "|**K=11**| 0.8035| 0.8035 | 0.8086| 0.8338 | \n",
    "\n",
    "|Precision|Euclidean Weighted|Manhattan Weighted|Euclidean|Manhattan|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|**K=1**| 0.6969| 0.6998 | 0.6969| 0.6998 | \n",
    "|**K=5**| 0.7153| 0.7156 | 0.7275| **0.7248** | \n",
    "|**K=9**| 0.7081| 0.7085 | 0.7024| 0.7009 |  \n",
    "|**K=11**| 0.7152| 0.7136 | 0.6978| 0.7058 | \n",
    "|Accuracy, Error|Euclidean Weighted|Manhattan Weighted|Euclidean|Manhattan|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|**K=1**|0.6317, 0.3683 | 0.6381, 0.3619 |0.6317, 0.3683|0.6381, 0.3619| \n",
    "|**K=5**| 0.6651, 0.3349 | 0.6683, 0.3317 |**0.6905, 0.3095**|0.6889, 0.3111| \n",
    "|**K=9**| 0.6619, 0.3381 | 0.6651, 0.3349 |0.6635, 0.3365|0.6619, 0.3381|  \n",
    "|**K=11**| 0.6746, 0.3254 | 0.673, 0.327 |0.6587, 0.3413|0.6762, 0.3238| \n",
    "\n",
    "|Average F1 Scores|Euclidean Weighted|Manhattan Weighted|Euclidean|Manhattan|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|**K=1**|0.7157| 0.722|0.7157| 0.722|   \n",
    "|**K=5**|0.7455| 0.7491|0.7681| **0.7678**|\n",
    "|**K=9**|0.7461| 0.7497|0.7518| 0.7509| \n",
    "|**K=11**|0.7568| 0.7559|0.7491| 0.7644 |  \n",
    "\n",
    "The proformance of the model shows that the most optimal value of K of K is k = 5, as well as using weight as uniform. But the Euclidean Distance has a better accuracy and error but the manhattan distance has a better recall, precision, and F1 score. I am going to choose the Manhattan as the optimal model, because the F1 score is more precise comapred to the accuracy of a model.\n",
    "\n",
    "\n",
    "\n",
    "|Average F1 Scores|Euclidean Weighted|Manhattan Weighted|Euclidean|Manhattan|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|**K=1**|0.7157| 0.722|0.7157| 0.722|   \n",
    "|**K=5**|0.7455| 0.7491|0.7681| **0.7678**|\n",
    "|**K=9**|0.7461| 0.7497|0.7518| 0.7509| \n",
    "|**K=11**|0.7568| 0.7559|0.7491| 0.7644 |  \n",
    "\n",
    "\n",
    "The overall proforomance of the model was adequite when looking at the average F1 scores. THe average F1 score that proformed most optimally was the Manhattan uniform metric with k = 5. But when looking at the ROC curve it seems that a threshold of .58 will make the values more optimal in order to get a better result. I attribute this to potentially the issues of the overall data. The data in itself had issues of simlarities within it, but after weeding those out it could've gotten rid of a major portion of data which could've helped with a more optimal classification.  \n",
    "\n",
    "\n",
    "|Confidence Interval 95%|Lower Bounds|Upper Bounds|\n",
    "|:---:|:---:|:---:|\n",
    "|**Fold 1**| 0.2823| 0.3471 |\n",
    "|**Fold 2**| 0.3207| 0.3875 |\n",
    "|**Fold 3**| 0.2835| 0.3485 |\n",
    "|**Fold 4**| 0.2531| 0.3162 |  \n",
    "|**Fold 5**| 0.2740| 0.3384 |\n",
    "\n",
    "The data provides insight on the effects of number of neighbors, distances, and weights on predicting binary outcomes. It appears the number of neighbors can effect the outcome to a major degree in different instances. Although being able to nail down the perfect value of number of neighbors is optimal because going past the optimal threshold of that number can result in worse values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
