{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Written Report\n",
    "\n",
    "# Summarizing the Dataset\n",
    "\n",
    "|       |   fixed acidity |   volatile acidity |   citric acid |   residual sugar |    chlorides |   free sulfur dioxide |   total sulfur dioxide |       density |          pH |   sulphates |    alcohol |     quality |\n",
    "|------|---------------|-------------------|--------------|-----------------|-------------|---------------------|-----------------------|--------------|------------|------------|-----------|------------|\n",
    "| count |     4898        |        4898        |   4898        |       4898       | 4898         |             4898      |              4898      | 4898          | 4898        | 4898        | 4898       | 4898        |\n",
    "| mean  |        6.85479  |           0.278241 |      0.334192 |          6.39141 |    0.0457724 |               35.3081 |               138.361  |    0.994027   |    3.18827  |    0.489847 |   10.5143  |    5.87791  |\n",
    "| std   |        0.843868 |           0.100795 |      0.12102  |          5.07206 |    0.021848  |               17.0071 |                42.4981 |    0.00299091 |    0.151001 |    0.114126 |    1.23062 |    0.885639 |\n",
    "| min   |        3.8      |           0.08     |      0        |          0.6     |    0.009     |                2      |                 9      |    0.98711    |    2.72     |    0.22     |    8       |    3        |\n",
    "| 25%   |        6.3      |           0.21     |      0.27     |          1.7     |    0.036     |               23      |               108      |    0.991723   |    3.09     |    0.41     |    9.5     |    5        |\n",
    "| 50%   |        6.8      |           0.26     |      0.32     |          5.2     |    0.043     |               34      |               134      |    0.99374    |    3.18     |    0.47     |   10.4     |    6        |\n",
    "| 75%   |        7.3      |           0.32     |      0.39     |          9.9     |    0.05      |               46      |               167      |    0.9961     |    3.28     |    0.55     |   11.4     |    6        |\n",
    "| max   |       14.2      |           1.1     |      1.66     |         65.8     |    0.346     |              289      |               440      |    1.03898    |    3.82     |    1.08     |   14.2     |    9        |\n",
    "\n",
    "# 12\n",
    "\n",
    "I decided to use feature scaling because it showed that through standardizing the values I got a better outcome when I regularlly used it. Another reason for this would be by feature scaling the values and numbers that are being used are not going to outragously high values especially when going to polynomial regression\n",
    "\n",
    "\n",
    "\n",
    "# 13\n",
    "\n",
    "I dropped features, the main reason for this was to get rid of values that are not needed because of havinga very weak correlation with the desicred predictor (quality). Because of this I dropped features pH, residual sugar, sulpahtes, citirc acid, and free sulter dioxide.\n",
    "\n",
    "\n",
    "|Features|Corelations|\n",
    "|------------------------|--------|\n",
    "|quality                 |1.000000|\n",
    "|alcohol                 |0.435575|\n",
    "|density                 |0.307123|\n",
    "|chlorides               |0.209934|\n",
    "|volatile acidity        |0.194723|\n",
    "|total sulfur dioxide    |0.174737|\n",
    "|fixed acidity           |0.113663|\n",
    "|pH                      |0.099427|\n",
    "|residual sugar         | 0.097577|\n",
    "|sulphates               |0.053678|\n",
    "|citric acid            | 0.009209|\n",
    "|free sulfur dioxide    | 0.008158|\n",
    "\n",
    "# 14\n",
    "\n",
    "The two types of Linear Regression (Closed form, and iterative optimization), have both their pros and cons. But for this specific dataset i would argue that Iterative optimization would be the most value form of Linear Regression to use. Iterative optimization utilizes and is able to efficently deal with a large dataset as we have ourselves. Closed form relies upon continuous changing of hyper parameters and measures in order to get an optimal output, while the iterative optimization does the optimaization based upon a certain set of values you input. \n",
    "\n",
    "# 15\n",
    "\n",
    "Batched Gradient decent and Stochastic gradient descent could learn similar outcomes as long as for the Stochastic gradient decent, implementation of lowering the learning rate at optimal times was implemented. But without that there is a low chance of Batched gradient decent and stochastic gradient descent having the same final results. Although they may have a similar value at a random point, I argue that they wouldn't both stop at the same time at the same value. Stochastic gradient descent takes random sampling from the data so its values it gets are sparatically placed, and on average the error of the function goes down but from point to point it can go up and down with the cost. Which in the long run could cause major issues. Also if the data sample is not convexed, then Batched gradient decent would get caught at local minimal instead of global minimal, while stochastic gradient descent could potentially pull out of that local minimum with its speratic sampling. \n",
    "\n",
    "# 16\n",
    "\n",
    "It would appear that my model is underfitting the data. At each seperate point on the learning curve, it shows that the training set higher than the validation set, This just means that the training model is expecting that value of MSE, while when we test it the validation is instead below that value which signifies underfitting of the data.\n",
    "\n",
    "Below is a table of the results from the learning curve using the hyper parameters that were deemed most optimal. \n",
    "\n",
    "|Size| RMSE For Train | RMSE for Validation|\n",
    "|---|-------------------|-------------------|\n",
    "|0.2| 0.92957751| 0.84045456|\n",
    "|0.4| 0.99142406 |0.92869396|\n",
    "|0.6| 0.99205841 | 0.96204668|\n",
    "|0.8| 0.99540749| 0.97375373|\n",
    "|1.0| 0.99353721 | 0.96875794|\n",
    "\n",
    "\n",
    "# 17\n",
    "\n",
    "Comared to the previous question it would seem that the learning curve is greatly underfitting the results. This is shown in the fact that both the training and validation results are so far apart from one another. As well is the error rate. \n",
    "\n",
    "|Size| RMSE For Train | RMSE for Validation|\n",
    "|---|-------------------|-------------------|\n",
    "|0.2|1.71876615| 1.14189784|\n",
    "|0.4| 2.08420077 |1.16975623|\n",
    "|0.6| 2.46452265 | 1.338151|\n",
    "|0.8| 2.40903871| 1.73599809|\n",
    "|1.0| 3.09805634 | 1.7721327 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
