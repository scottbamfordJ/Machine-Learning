{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_outputs):\n",
    "    network = list()\n",
    "    n_inputs= n_inputs.shape[1]\n",
    "    n_outputs = n_outputs.shape[0]\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs)]} for i in range(1)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    logistic_values = 1 / (1 + exp(-z))\n",
    "    return logistic_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    relu_values = max(0,z)\n",
    "    return relu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    tanh_values = np.tanh(z)\n",
    "    return tanh_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_splits(X, y, test_size):\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size, random_state=42)\n",
    "    return X_train, X_validation, y_train, y_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL40lEQVR4nO3dW4hd9RXH8d+vY7xGSaxWJBHtSAmIUHNBKgFpNYpWsS81RFCotCQPrRha0NiX4ptPYh+KELxU8IajBoq01gQVEVrtTIz1MrFoiJhEHSWRGAsR4+rD2SkxnTp7xv3/z5mzvh845MzMmb3WzOR39t7n7L2XI0IABtu3ZrsBAOURdCABgg4kQNCBBAg6kABBBxLoi6DbvsL2W7bftr2hcK37bE/Yfr1knSPqnWX7Odvjtt+wfXPhesfbftn2q02920vWa2oO2X7F9lOlazX1dtp+zfY226OFay2w/bjt7c3f8KKCtZY0P9Ph237b6ztZeETM6k3SkKR3JA1LOlbSq5LOK1jvYknLJL1e6ec7U9Ky5v7Jkv5V+OezpPnN/XmSXpL0g8I/468lPSzpqUq/052STqtU6wFJv2juHytpQaW6Q5I+kHR2F8vrhzX6hZLejogdEfG5pEcl/aRUsYh4QdLeUsufpN77EbG1uf+ppHFJiwrWi4g40Hw4r7kVOyrK9mJJV0m6p1SN2WL7FPVWDPdKUkR8HhGfVCp/qaR3IuLdLhbWD0FfJOm9Iz7epYJBmE22z5G0VL21bMk6Q7a3SZqQtDkiSta7S9Itkr4sWONoIekZ22O21xasMyzpI0n3N7sm99g+qWC9I62R9EhXC+uHoHuSzw3ccbm250t6QtL6iNhfslZEHIqICyQtlnSh7fNL1LF9taSJiBgrsfyvsTIilkm6UtIvbV9cqM4x6u3m3R0RSyV9Jqnoa0iSZPtYSddIGulqmf0Q9F2Szjri48WS9sxSL0XYnqdeyB+KiCdr1W02M5+XdEWhEislXWN7p3q7XJfYfrBQrf+KiD3NvxOSNqm3+1fCLkm7jtgiely94Jd2paStEfFhVwvsh6D/Q9L3bH+3eSZbI+lPs9xTZ2xbvX288Yi4s0K9020vaO6fIGmVpO0lakXEbRGxOCLOUe/v9mxEXF+i1mG2T7J98uH7ki6XVOQdlIj4QNJ7tpc0n7pU0pslah3lOnW42S71Nk1mVUR8YftXkv6q3iuN90XEG6Xq2X5E0g8lnWZ7l6TfRcS9peqpt9a7QdJrzX6zJP02Iv5cqN6Zkh6wPaTeE/ljEVHlba9KzpC0qff8qWMkPRwRTxesd5Okh5qV0A5JNxasJdsnSrpM0rpOl9u8lA9ggPXDpjuAwgg6kABBBxIg6EACBB1IoK+CXvhwxlmrRT3qzXa9vgq6pJq/zKp/OOpRbzbr9VvQARRQ5IAZ2wN9FM7ChQun/T0HDx7UcccdN6N6ixZN/2S+vXv36tRTT51Rvf37p3/OzYEDBzR//vwZ1du9e/e0vyci1BwdN22HDh2a0ffNFRHxP7+YWT8Edi5atWpV1Xp33HFH1XpbtmypWm/DhuInhH3Fvn37qtbrB2y6AwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IoFXQa45MAtC9KYPeXGTwD+pdgvY8SdfZPq90YwC602aNXnVkEoDutQl6mpFJwKBqc1JLq5FJzYnytc/ZBdBCm6C3GpkUERslbZQG/zRVYK5ps+k+0COTgAymXKPXHpkEoHutLjzRzAkrNSsMQGEcGQckQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IAEmtcxA7ckpw8PDVevNZOTUN7F3796q9VavXl213sjISNV6k2GNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQTajGS6z/aE7ddrNASge23W6H+UdEXhPgAUNGXQI+IFSXXPOgDQKfbRgQQ6O02V2WtA/+os6MxeA/oXm+5AAm3eXntE0t8kLbG9y/bPy7cFoEtthixeV6MRAOWw6Q4kQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IIGBmL22fPnyqvVqz0I799xzq9bbsWNH1XqbN2+uWq/2/xdmrwGogqADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJtLk45Fm2n7M9bvsN2zfXaAxAd9oc6/6FpN9ExFbbJ0sas705It4s3BuAjrSZvfZ+RGxt7n8qaVzSotKNAejOtPbRbZ8jaamkl4p0A6CI1qep2p4v6QlJ6yNi/yRfZ/Ya0KdaBd32PPVC/lBEPDnZY5i9BvSvNq+6W9K9ksYj4s7yLQHoWpt99JWSbpB0ie1tze3HhfsC0KE2s9delOQKvQAohCPjgAQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kMBCz1xYuXFi13tjYWNV6tWeh1Vb795kRa3QgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4k0OYqsMfbftn2q83stdtrNAagO22OdT8o6ZKIONBc3/1F23+JiL8X7g1AR9pcBTYkHWg+nNfcGNAAzCGt9tFtD9neJmlC0uaIYPYaMIe0CnpEHIqICyQtlnSh7fOPfozttbZHbY923COAb2har7pHxCeSnpd0xSRf2xgRKyJiRTetAehKm1fdT7e9oLl/gqRVkrYX7gtAh9q86n6mpAdsD6n3xPBYRDxVti0AXWrzqvs/JS2t0AuAQjgyDkiAoAMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAsxem4EtW7ZUrTfoav/99u3bV7VeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaB70Z4vCKbS4MCcwx01mj3yxpvFQjAMppO5JpsaSrJN1Tth0AJbRdo98l6RZJX5ZrBUApbSa1XC1pIiLGpngcs9eAPtVmjb5S0jW2d0p6VNIlth88+kHMXgP615RBj4jbImJxRJwjaY2kZyPi+uKdAegM76MDCUzrUlIR8bx6Y5MBzCGs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJDAQs9dqz9Javnx51Xq11Z6FVvv3OTIyUrVeP2CNDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQRaHQLbXOr5U0mHJH3BJZ2BuWU6x7r/KCI+LtYJgGLYdAcSaBv0kPSM7THba0s2BKB7bTfdV0bEHtvfkbTZ9vaIeOHIBzRPADwJAH2o1Ro9IvY0/05I2iTpwkkew+w1oE+1maZ6ku2TD9+XdLmk10s3BqA7bTbdz5C0yfbhxz8cEU8X7QpAp6YMekTskPT9Cr0AKIS314AECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJOCI6H6hdvcL/RrDw8M1y2l0dLRqvXXr1lWtd+2111atV/vvt2LFYJ+OERE++nOs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAq6DbXmD7cdvbbY/bvqh0YwC603aAw+8lPR0RP7V9rKQTC/YEoGNTBt32KZIulvQzSYqIzyV9XrYtAF1qs+k+LOkjSffbfsX2Pc0gh6+wvdb2qO26p3YBmFKboB8jaZmkuyNiqaTPJG04+kGMZAL6V5ug75K0KyJeaj5+XL3gA5gjpgx6RHwg6T3bS5pPXSrpzaJdAehU21fdb5L0UPOK+w5JN5ZrCUDXWgU9IrZJYt8bmKM4Mg5IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIDMXuttrVr11atd+utt1atNzY2VrXe6tWrq9YbdMxeA5Ii6EACBB1IgKADCRB0IAGCDiRA0IEECDqQAEEHEpgy6LaX2N52xG2/7fUVegPQkSmvGRcRb0m6QJJsD0naLWlT2bYAdGm6m+6XSnonIt4t0QyAMqYb9DWSHinRCIByWge9uab7NZJG/s/Xmb0G9Km2Axwk6UpJWyPiw8m+GBEbJW2UBv80VWCumc6m+3Visx2Yk1oF3faJki6T9GTZdgCU0HYk078lfbtwLwAK4cg4IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQggVKz1z6SNJNz1k+T9HHH7fRDLepRr1a9syPi9KM/WSToM2V7NCJWDFot6lFvtuux6Q4kQNCBBPot6BsHtBb1qDer9fpqHx1AGf22RgdQAEEHEiDoQAIEHUiAoAMJ/AchD47vy2xCkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# fit only on the training data\n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'hidden_layer_sizes': [(3,), (5,), (10,)], 'solver':['sgd'],\n",
    "              'alpha': [0.0001,0.001,0.01,0.1], 'activation' : ['logistic','tanh', 'relu'],  \n",
    "              'learning_rate' : ['constant' , 'adaptive'],\n",
    "              'learning_rate_init' :[0.001,0.01,0.1],\n",
    "              'max_iter' : [1000,800,600,400,200],\n",
    "              'tol' : [0.0001, 0.001, 0.01, 0.1],\n",
    "              'verbose': [3],\n",
    "              'early_stopping':[True],\n",
    "              'n_iter_no_change' : [10,5,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12960 candidates, totalling 25920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 612 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=4)]: Done 1309 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=4)]: Done 1972 tasks      | elapsed:   45.9s\n",
      "[Parallel(n_jobs=4)]: Done 2920 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 4324 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 5776 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done 7676 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done 9360 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done 11740 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done 14428 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=4)]: Done 17372 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=4)]: Done 20572 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=4)]: Done 24028 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=4)]: Done 25920 out of 25920 | elapsed: 11.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04936330\n",
      "Validation score: 0.638889\n",
      "Iteration 2, loss = 1.18852494\n",
      "Validation score: 0.798611\n",
      "Iteration 3, loss = 0.76983102\n",
      "Validation score: 0.826389\n",
      "Iteration 4, loss = 0.52413644\n",
      "Validation score: 0.847222\n",
      "Iteration 5, loss = 0.37517798\n",
      "Validation score: 0.861111\n",
      "Iteration 6, loss = 0.28915271\n",
      "Validation score: 0.875000\n",
      "Iteration 7, loss = 0.23525373\n",
      "Validation score: 0.868056\n",
      "Iteration 8, loss = 0.19820975\n",
      "Validation score: 0.868056\n",
      "Iteration 9, loss = 0.16938042\n",
      "Validation score: 0.888889\n",
      "Iteration 10, loss = 0.14842460\n",
      "Validation score: 0.888889\n",
      "Iteration 11, loss = 0.13145707\n",
      "Validation score: 0.888889\n",
      "Iteration 12, loss = 0.11741819\n",
      "Validation score: 0.881944\n",
      "Iteration 13, loss = 0.10690119\n",
      "Validation score: 0.881944\n",
      "Iteration 14, loss = 0.09885464\n",
      "Validation score: 0.888889\n",
      "Iteration 15, loss = 0.09176746\n",
      "Validation score: 0.881944\n",
      "Iteration 16, loss = 0.08566708\n",
      "Validation score: 0.888889\n",
      "Iteration 17, loss = 0.08025969\n",
      "Validation score: 0.888889\n",
      "Iteration 18, loss = 0.07557823\n",
      "Validation score: 0.888889\n",
      "Iteration 19, loss = 0.07164444\n",
      "Validation score: 0.888889\n",
      "Iteration 20, loss = 0.06785735\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 21, loss = 0.06385257\n",
      "Validation score: 0.895833\n",
      "Iteration 22, loss = 0.06214021\n",
      "Validation score: 0.895833\n",
      "Iteration 23, loss = 0.06107226\n",
      "Validation score: 0.895833\n",
      "Iteration 24, loss = 0.06026065\n",
      "Validation score: 0.895833\n",
      "Iteration 25, loss = 0.05950362\n",
      "Validation score: 0.895833\n",
      "Iteration 26, loss = 0.05882198\n",
      "Validation score: 0.888889\n",
      "Iteration 27, loss = 0.05830839\n",
      "Validation score: 0.888889\n",
      "Iteration 28, loss = 0.05771200\n",
      "Validation score: 0.888889\n",
      "Iteration 29, loss = 0.05721448\n",
      "Validation score: 0.888889\n",
      "Iteration 30, loss = 0.05671502\n",
      "Validation score: 0.888889\n",
      "Iteration 31, loss = 0.05621681\n",
      "Validation score: 0.888889\n",
      "Iteration 32, loss = 0.05577353\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 33, loss = 0.05522382\n",
      "Validation score: 0.888889\n",
      "Iteration 34, loss = 0.05494049\n",
      "Validation score: 0.888889\n",
      "Iteration 35, loss = 0.05477008\n",
      "Validation score: 0.888889\n",
      "Iteration 36, loss = 0.05464302\n",
      "Validation score: 0.888889\n",
      "Iteration 37, loss = 0.05453104\n",
      "Validation score: 0.888889\n",
      "Iteration 38, loss = 0.05444382\n",
      "Validation score: 0.888889\n",
      "Iteration 39, loss = 0.05434505\n",
      "Validation score: 0.888889\n",
      "Iteration 40, loss = 0.05425554\n",
      "Validation score: 0.888889\n",
      "Iteration 41, loss = 0.05416619\n",
      "Validation score: 0.888889\n",
      "Iteration 42, loss = 0.05407772\n",
      "Validation score: 0.888889\n",
      "Iteration 43, loss = 0.05399149\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 44, loss = 0.05388502\n",
      "Validation score: 0.888889\n",
      "Iteration 45, loss = 0.05383560\n",
      "Validation score: 0.888889\n",
      "Iteration 46, loss = 0.05380542\n",
      "Validation score: 0.888889\n",
      "Iteration 47, loss = 0.05377938\n",
      "Validation score: 0.888889\n",
      "Iteration 48, loss = 0.05376051\n",
      "Validation score: 0.888889\n",
      "Iteration 49, loss = 0.05374134\n",
      "Validation score: 0.888889\n",
      "Iteration 50, loss = 0.05372229\n",
      "Validation score: 0.888889\n",
      "Iteration 51, loss = 0.05370570\n",
      "Validation score: 0.888889\n",
      "Iteration 52, loss = 0.05368886\n",
      "Validation score: 0.888889\n",
      "Iteration 53, loss = 0.05367108\n",
      "Validation score: 0.888889\n",
      "Iteration 54, loss = 0.05365425\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 55, loss = 0.05363324\n",
      "Validation score: 0.888889\n",
      "Iteration 56, loss = 0.05362368\n",
      "Validation score: 0.888889\n",
      "Iteration 57, loss = 0.05361729\n",
      "Validation score: 0.888889\n",
      "Iteration 58, loss = 0.05361211\n",
      "Validation score: 0.888889\n",
      "Iteration 59, loss = 0.05360769\n",
      "Validation score: 0.888889\n",
      "Iteration 60, loss = 0.05360433\n",
      "Validation score: 0.888889\n",
      "Iteration 61, loss = 0.05360100\n",
      "Validation score: 0.888889\n",
      "Iteration 62, loss = 0.05359714\n",
      "Validation score: 0.888889\n",
      "Iteration 63, loss = 0.05359362\n",
      "Validation score: 0.888889\n",
      "Iteration 64, loss = 0.05359013\n",
      "Validation score: 0.888889\n",
      "Iteration 65, loss = 0.05358702\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 66, loss = 0.05358270\n",
      "Validation score: 0.888889\n",
      "Iteration 67, loss = 0.05358078\n",
      "Validation score: 0.888889\n",
      "Iteration 68, loss = 0.05357972\n",
      "Validation score: 0.888889\n",
      "Iteration 69, loss = 0.05357859\n",
      "Validation score: 0.888889\n",
      "Iteration 70, loss = 0.05357779\n",
      "Validation score: 0.888889\n",
      "Iteration 71, loss = 0.05357705\n",
      "Validation score: 0.888889\n",
      "Iteration 72, loss = 0.05357635\n",
      "Validation score: 0.888889\n",
      "Iteration 73, loss = 0.05357571\n",
      "Validation score: 0.888889\n",
      "Iteration 74, loss = 0.05357503\n",
      "Validation score: 0.888889\n",
      "Iteration 75, loss = 0.05357438\n",
      "Validation score: 0.888889\n",
      "Iteration 76, loss = 0.05357362\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 77, loss = 0.05357281\n",
      "Validation score: 0.888889\n",
      "Iteration 78, loss = 0.05357243\n",
      "Validation score: 0.888889\n",
      "Iteration 79, loss = 0.05357217\n",
      "Validation score: 0.888889\n",
      "Iteration 80, loss = 0.05357197\n",
      "Validation score: 0.888889\n",
      "Iteration 81, loss = 0.05357181\n",
      "Validation score: 0.888889\n",
      "Iteration 82, loss = 0.05357166\n",
      "Validation score: 0.888889\n",
      "Iteration 83, loss = 0.05357151\n",
      "Validation score: 0.888889\n",
      "Iteration 84, loss = 0.05357139\n",
      "Validation score: 0.888889\n",
      "Iteration 85, loss = 0.05357124\n",
      "Validation score: 0.888889\n",
      "Iteration 86, loss = 0.05357111\n",
      "Validation score: 0.888889\n",
      "Iteration 87, loss = 0.05357098\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 88, loss = 0.05357080\n",
      "Validation score: 0.888889\n",
      "Iteration 89, loss = 0.05357073\n",
      "Validation score: 0.888889\n",
      "Iteration 90, loss = 0.05357068\n",
      "Validation score: 0.888889\n",
      "Iteration 91, loss = 0.05357064\n",
      "Validation score: 0.888889\n",
      "Iteration 92, loss = 0.05357061\n",
      "Validation score: 0.888889\n",
      "Iteration 93, loss = 0.05357058\n",
      "Validation score: 0.888889\n",
      "Iteration 94, loss = 0.05357055\n",
      "Validation score: 0.888889\n",
      "Iteration 95, loss = 0.05357052\n",
      "Validation score: 0.888889\n",
      "Iteration 96, loss = 0.05357050\n",
      "Validation score: 0.888889\n",
      "Iteration 97, loss = 0.05357047\n",
      "Validation score: 0.888889\n",
      "Iteration 98, loss = 0.05357044\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 99, loss = 0.05357041\n",
      "Validation score: 0.888889\n",
      "Iteration 100, loss = 0.05357039\n",
      "Validation score: 0.888889\n",
      "Iteration 101, loss = 0.05357038\n",
      "Validation score: 0.888889\n",
      "Iteration 102, loss = 0.05357037\n",
      "Validation score: 0.888889\n",
      "Iteration 103, loss = 0.05357037\n",
      "Validation score: 0.888889\n",
      "Iteration 104, loss = 0.05357036\n",
      "Validation score: 0.888889\n",
      "Iteration 105, loss = 0.05357035\n",
      "Validation score: 0.888889\n",
      "Iteration 106, loss = 0.05357035\n",
      "Validation score: 0.888889\n",
      "Iteration 107, loss = 0.05357034\n",
      "Validation score: 0.888889\n",
      "Iteration 108, loss = 0.05357034\n",
      "Validation score: 0.888889\n",
      "Iteration 109, loss = 0.05357033\n",
      "Validation score: 0.888889\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=MLPClassifier(), n_jobs=4,\n",
       "             param_grid={'activation': ['logistic', 'tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
       "                         'early_stopping': [True],\n",
       "                         'hidden_layer_sizes': [(3,), (5,), (10,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'learning_rate_init': [0.001, 0.01, 0.1],\n",
       "                         'max_iter': [1000, 800, 600, 400, 200],\n",
       "                         'n_iter_no_change': [10, 5, 2], 'solver': ['sgd'],\n",
       "                         'tol': [0.0001, 0.001, 0.01, 0.1], 'verbose': [3]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Used this code from the workbook\n",
    "\n",
    "clf_mlp_cv = GridSearchCV(clf_mlp, param_grid, scoring='accuracy', cv=2, verbose=3, n_jobs=4)\n",
    "clf_mlp_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.942241\n",
      "Optimal Hyperparameter Values:  {'activation': 'tanh', 'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 600, 'n_iter_no_change': 10, 'solver': 'sgd', 'tol': 0.001, 'verbose': 3}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params_optimal_mlp = clf_mlp_cv.best_params_\n",
    "\n",
    "print(\"Best Score (accuracy): %f\" % clf_mlp_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_mlp)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Hyperparameter Values:  {'activation': 'tanh', 'alpha': 0.001, 'early_stopping': True, 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 600, 'n_iter_no_change': 10, 'solver': 'sgd', 'tol': 0.001, 'verbose': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_optimal = { 'hidden_layer_sizes': [(10,)], 'solver':['sgd'],\n",
    "              'alpha': [0.001], 'activation' : ['tanh'],  \n",
    "              'learning_rate' : ['adaptive'],\n",
    "              'learning_rate_init': [0.1],\n",
    "              'max_iter' : [600],\n",
    "              'tol' : [0.001],\n",
    "              'verbose': [10],\n",
    "              'early_stopping':[True],\n",
    "              'n_iter_no_change' : [10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32677393\n",
      "Validation score: 0.173611\n",
      "Iteration 2, loss = 2.14706435\n",
      "Validation score: 0.534722\n",
      "Iteration 3, loss = 1.94559840\n",
      "Validation score: 0.694444\n",
      "Iteration 4, loss = 1.68449718\n",
      "Validation score: 0.791667\n",
      "Iteration 5, loss = 1.42182684\n",
      "Validation score: 0.833333\n",
      "Iteration 6, loss = 1.19175239\n",
      "Validation score: 0.847222\n",
      "Iteration 7, loss = 1.00276736\n",
      "Validation score: 0.881944\n",
      "Iteration 8, loss = 0.85476246\n",
      "Validation score: 0.902778\n",
      "Iteration 9, loss = 0.73406049\n",
      "Validation score: 0.923611\n",
      "Iteration 10, loss = 0.63650775\n",
      "Validation score: 0.944444\n",
      "Iteration 11, loss = 0.55514530\n",
      "Validation score: 0.958333\n",
      "Iteration 12, loss = 0.48990015\n",
      "Validation score: 0.958333\n",
      "Iteration 13, loss = 0.43657108\n",
      "Validation score: 0.965278\n",
      "Iteration 14, loss = 0.39259567\n",
      "Validation score: 0.979167\n",
      "Iteration 15, loss = 0.35730188\n",
      "Validation score: 0.979167\n",
      "Iteration 16, loss = 0.32756785\n",
      "Validation score: 0.979167\n",
      "Iteration 17, loss = 0.30264980\n",
      "Validation score: 0.979167\n",
      "Iteration 18, loss = 0.28125727\n",
      "Validation score: 0.986111\n",
      "Iteration 19, loss = 0.26350110\n",
      "Validation score: 0.986111\n",
      "Iteration 20, loss = 0.24753891\n",
      "Validation score: 0.986111\n",
      "Iteration 21, loss = 0.23361628\n",
      "Validation score: 0.986111\n",
      "Iteration 22, loss = 0.22131715\n",
      "Validation score: 0.993056\n",
      "Iteration 23, loss = 0.21056192\n",
      "Validation score: 0.986111\n",
      "Iteration 24, loss = 0.20070928\n",
      "Validation score: 0.986111\n",
      "Iteration 25, loss = 0.19216050\n",
      "Validation score: 0.986111\n",
      "Iteration 26, loss = 0.18459266\n",
      "Validation score: 0.986111\n",
      "Iteration 27, loss = 0.17685610\n",
      "Validation score: 0.979167\n",
      "Iteration 28, loss = 0.17056956\n",
      "Validation score: 0.986111\n",
      "Iteration 29, loss = 0.16445154\n",
      "Validation score: 0.986111\n",
      "Iteration 30, loss = 0.15892238\n",
      "Validation score: 0.986111\n",
      "Iteration 31, loss = 0.15382091\n",
      "Validation score: 0.986111\n",
      "Iteration 32, loss = 0.14880128\n",
      "Validation score: 0.986111\n",
      "Iteration 33, loss = 0.14479297\n",
      "Validation score: 0.979167\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 34, loss = 0.14012744\n",
      "Validation score: 0.979167\n",
      "Iteration 35, loss = 0.13778212\n",
      "Validation score: 0.986111\n",
      "Iteration 36, loss = 0.13627291\n",
      "Validation score: 0.986111\n",
      "Iteration 37, loss = 0.13518908\n",
      "Validation score: 0.986111\n",
      "Iteration 38, loss = 0.13426306\n",
      "Validation score: 0.986111\n",
      "Iteration 39, loss = 0.13342030\n",
      "Validation score: 0.986111\n",
      "Iteration 40, loss = 0.13266563\n",
      "Validation score: 0.986111\n",
      "Iteration 41, loss = 0.13194267\n",
      "Validation score: 0.986111\n",
      "Iteration 42, loss = 0.13125415\n",
      "Validation score: 0.986111\n",
      "Iteration 43, loss = 0.13056539\n",
      "Validation score: 0.986111\n",
      "Iteration 44, loss = 0.12985547\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 45, loss = 0.12915965\n",
      "Validation score: 0.986111\n",
      "Iteration 46, loss = 0.12878460\n",
      "Validation score: 0.986111\n",
      "Iteration 47, loss = 0.12851224\n",
      "Validation score: 0.979167\n",
      "Iteration 48, loss = 0.12833166\n",
      "Validation score: 0.979167\n",
      "Iteration 49, loss = 0.12817465\n",
      "Validation score: 0.979167\n",
      "Iteration 50, loss = 0.12803121\n",
      "Validation score: 0.986111\n",
      "Iteration 51, loss = 0.12789200\n",
      "Validation score: 0.986111\n",
      "Iteration 52, loss = 0.12775847\n",
      "Validation score: 0.986111\n",
      "Iteration 53, loss = 0.12762472\n",
      "Validation score: 0.986111\n",
      "Iteration 54, loss = 0.12750128\n",
      "Validation score: 0.986111\n",
      "Iteration 55, loss = 0.12736659\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 56, loss = 0.12722771\n",
      "Validation score: 0.986111\n",
      "Iteration 57, loss = 0.12715753\n",
      "Validation score: 0.986111\n",
      "Iteration 58, loss = 0.12711026\n",
      "Validation score: 0.986111\n",
      "Iteration 59, loss = 0.12707169\n",
      "Validation score: 0.986111\n",
      "Iteration 60, loss = 0.12704140\n",
      "Validation score: 0.986111\n",
      "Iteration 61, loss = 0.12701266\n",
      "Validation score: 0.986111\n",
      "Iteration 62, loss = 0.12698656\n",
      "Validation score: 0.986111\n",
      "Iteration 63, loss = 0.12695879\n",
      "Validation score: 0.986111\n",
      "Iteration 64, loss = 0.12693540\n",
      "Validation score: 0.986111\n",
      "Iteration 65, loss = 0.12690823\n",
      "Validation score: 0.986111\n",
      "Iteration 66, loss = 0.12688206\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 67, loss = 0.12685487\n",
      "Validation score: 0.986111\n",
      "Iteration 68, loss = 0.12684081\n",
      "Validation score: 0.986111\n",
      "Iteration 69, loss = 0.12683105\n",
      "Validation score: 0.986111\n",
      "Iteration 70, loss = 0.12682384\n",
      "Validation score: 0.986111\n",
      "Iteration 71, loss = 0.12681792\n",
      "Validation score: 0.986111\n",
      "Iteration 72, loss = 0.12681236\n",
      "Validation score: 0.986111\n",
      "Iteration 73, loss = 0.12680666\n",
      "Validation score: 0.986111\n",
      "Iteration 74, loss = 0.12680179\n",
      "Validation score: 0.986111\n",
      "Iteration 75, loss = 0.12679633\n",
      "Validation score: 0.986111\n",
      "Iteration 76, loss = 0.12679144\n",
      "Validation score: 0.986111\n",
      "Iteration 77, loss = 0.12678624\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 78, loss = 0.12678075\n",
      "Validation score: 0.986111\n",
      "Iteration 79, loss = 0.12677785\n",
      "Validation score: 0.986111\n",
      "Iteration 80, loss = 0.12677586\n",
      "Validation score: 0.986111\n",
      "Iteration 81, loss = 0.12677447\n",
      "Validation score: 0.986111\n",
      "Iteration 82, loss = 0.12677319\n",
      "Validation score: 0.986111\n",
      "Iteration 83, loss = 0.12677207\n",
      "Validation score: 0.986111\n",
      "Iteration 84, loss = 0.12677102\n",
      "Validation score: 0.986111\n",
      "Iteration 85, loss = 0.12676997\n",
      "Validation score: 0.986111\n",
      "Iteration 86, loss = 0.12676895\n",
      "Validation score: 0.986111\n",
      "Iteration 87, loss = 0.12676790\n",
      "Validation score: 0.986111\n",
      "Iteration 88, loss = 0.12676683\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 89, loss = 0.12676581\n",
      "Validation score: 0.986111\n",
      "Iteration 90, loss = 0.12676519\n",
      "Validation score: 0.986111\n",
      "Iteration 91, loss = 0.12676481\n",
      "Validation score: 0.986111\n",
      "Iteration 92, loss = 0.12676451\n",
      "Validation score: 0.986111\n",
      "Iteration 93, loss = 0.12676428\n",
      "Validation score: 0.986111\n",
      "Iteration 94, loss = 0.12676405\n",
      "Validation score: 0.986111\n",
      "Iteration 95, loss = 0.12676383\n",
      "Validation score: 0.986111\n",
      "Iteration 96, loss = 0.12676363\n",
      "Validation score: 0.986111\n",
      "Iteration 97, loss = 0.12676342\n",
      "Validation score: 0.986111\n",
      "Iteration 98, loss = 0.12676321\n",
      "Validation score: 0.986111\n",
      "Iteration 99, loss = 0.12676301\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 100, loss = 0.12676279\n",
      "Validation score: 0.986111\n",
      "Iteration 101, loss = 0.12676267\n",
      "Validation score: 0.986111\n",
      "Iteration 102, loss = 0.12676260\n",
      "Validation score: 0.986111\n",
      "Iteration 103, loss = 0.12676254\n",
      "Validation score: 0.986111\n",
      "Iteration 104, loss = 0.12676249\n",
      "Validation score: 0.986111\n",
      "Iteration 105, loss = 0.12676244\n",
      "Validation score: 0.986111\n",
      "Iteration 106, loss = 0.12676240\n",
      "Validation score: 0.986111\n",
      "Iteration 107, loss = 0.12676236\n",
      "Validation score: 0.986111\n",
      "Iteration 108, loss = 0.12676232\n",
      "Validation score: 0.986111\n",
      "Iteration 109, loss = 0.12676228\n",
      "Validation score: 0.986111\n",
      "Iteration 110, loss = 0.12676224\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 111, loss = 0.12676219\n",
      "Validation score: 0.986111\n",
      "Iteration 112, loss = 0.12676217\n",
      "Validation score: 0.986111\n",
      "Iteration 113, loss = 0.12676215\n",
      "Validation score: 0.986111\n",
      "Iteration 114, loss = 0.12676214\n",
      "Validation score: 0.986111\n",
      "Iteration 115, loss = 0.12676213\n",
      "Validation score: 0.986111\n",
      "Iteration 116, loss = 0.12676212\n",
      "Validation score: 0.986111\n",
      "Iteration 117, loss = 0.12676211\n",
      "Validation score: 0.986111\n",
      "Iteration 118, loss = 0.12676210\n",
      "Validation score: 0.986111\n",
      "Iteration 119, loss = 0.12676210\n",
      "Validation score: 0.986111\n",
      "Iteration 120, loss = 0.12676209\n",
      "Validation score: 0.986111\n",
      "Iteration 121, loss = 0.12676208\n",
      "Validation score: 0.986111\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(10,), learning_rate='adaptive',\n",
       "              learning_rate_init=0.1, max_iter=600, random_state=42,\n",
       "              solver='sgd', tol=0.001, verbose=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(random_state=42, hidden_layer_sizes=(10, ), alpha=0.001, solver='sgd',\n",
    "                      max_iter=600, n_iter_no_change = 10, learning_rate='adaptive',learning_rate_init = 0.1, activation='logistic', tol = 0.001,\n",
    "                    verbose=10, early_stopping = True)\n",
    "mlp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores = mlp_clf.validation_scores_\n",
    "validation_loss = mlp_clf.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31680675\n",
      "Iteration 2, loss = 2.11073239\n",
      "Iteration 3, loss = 1.85807125\n",
      "Iteration 4, loss = 1.55304091\n",
      "Iteration 5, loss = 1.27046769\n",
      "Iteration 6, loss = 1.04303337\n",
      "Iteration 7, loss = 0.86724393\n",
      "Iteration 8, loss = 0.72960734\n",
      "Iteration 9, loss = 0.61883085\n",
      "Iteration 10, loss = 0.53335840\n",
      "Iteration 11, loss = 0.46431562\n",
      "Iteration 12, loss = 0.40877098\n",
      "Iteration 13, loss = 0.36551199\n",
      "Iteration 14, loss = 0.32995993\n",
      "Iteration 15, loss = 0.30161662\n",
      "Iteration 16, loss = 0.27800450\n",
      "Iteration 17, loss = 0.25751772\n",
      "Iteration 18, loss = 0.24108035\n",
      "Iteration 19, loss = 0.22673658\n",
      "Iteration 20, loss = 0.21412046\n",
      "Iteration 21, loss = 0.20293742\n",
      "Iteration 22, loss = 0.19308349\n",
      "Iteration 23, loss = 0.18426923\n",
      "Iteration 24, loss = 0.17656102\n",
      "Iteration 25, loss = 0.16912276\n",
      "Iteration 26, loss = 0.16268820\n",
      "Iteration 27, loss = 0.15678801\n",
      "Iteration 28, loss = 0.15088539\n",
      "Iteration 29, loss = 0.14587784\n",
      "Iteration 30, loss = 0.14101754\n",
      "Iteration 31, loss = 0.13672817\n",
      "Iteration 32, loss = 0.13278614\n",
      "Iteration 33, loss = 0.12889304\n",
      "Iteration 34, loss = 0.12505022\n",
      "Iteration 35, loss = 0.12159634\n",
      "Iteration 36, loss = 0.11847140\n",
      "Iteration 37, loss = 0.11556273\n",
      "Iteration 38, loss = 0.11266975\n",
      "Iteration 39, loss = 0.11001495\n",
      "Iteration 40, loss = 0.10740410\n",
      "Iteration 41, loss = 0.10494645\n",
      "Iteration 42, loss = 0.10277112\n",
      "Iteration 43, loss = 0.10043252\n",
      "Iteration 44, loss = 0.09808229\n",
      "Iteration 45, loss = 0.09588110\n",
      "Iteration 46, loss = 0.09394964\n",
      "Iteration 47, loss = 0.09201512\n",
      "Iteration 48, loss = 0.08982671\n",
      "Iteration 49, loss = 0.08830600\n",
      "Iteration 50, loss = 0.08653554\n",
      "Iteration 51, loss = 0.08438693\n",
      "Iteration 52, loss = 0.08291470\n",
      "Iteration 53, loss = 0.08113748\n",
      "Iteration 54, loss = 0.07952699\n",
      "Iteration 55, loss = 0.07770096\n",
      "Iteration 56, loss = 0.07624590\n",
      "Iteration 57, loss = 0.07457217\n",
      "Iteration 58, loss = 0.07349928\n",
      "Iteration 59, loss = 0.07225354\n",
      "Iteration 60, loss = 0.07107490\n",
      "Iteration 61, loss = 0.06992542\n",
      "Iteration 62, loss = 0.06855146\n",
      "Iteration 63, loss = 0.06759786\n",
      "Iteration 64, loss = 0.06644576\n",
      "Iteration 65, loss = 0.06572877\n",
      "Iteration 66, loss = 0.06482791\n",
      "Iteration 67, loss = 0.06353192\n",
      "Iteration 68, loss = 0.06243969\n",
      "Iteration 69, loss = 0.06158304\n",
      "Iteration 70, loss = 0.06079530\n",
      "Iteration 71, loss = 0.05994970\n",
      "Iteration 72, loss = 0.05911512\n",
      "Iteration 73, loss = 0.05829429\n",
      "Iteration 74, loss = 0.05764038\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 75, loss = 0.05663065\n",
      "Iteration 76, loss = 0.05622247\n",
      "Iteration 77, loss = 0.05596339\n",
      "Iteration 78, loss = 0.05575154\n",
      "Iteration 79, loss = 0.05559862\n",
      "Iteration 80, loss = 0.05542939\n",
      "Iteration 81, loss = 0.05529633\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 82, loss = 0.05513348\n",
      "Iteration 83, loss = 0.05503971\n",
      "Iteration 84, loss = 0.05499573\n",
      "Iteration 85, loss = 0.05495802\n",
      "Iteration 86, loss = 0.05492411\n",
      "Iteration 87, loss = 0.05489663\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 88, loss = 0.05486048\n",
      "Iteration 89, loss = 0.05484523\n",
      "Iteration 90, loss = 0.05483602\n",
      "Iteration 91, loss = 0.05482883\n",
      "Iteration 92, loss = 0.05482205\n",
      "Iteration 93, loss = 0.05481529\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 94, loss = 0.05480779\n",
      "Iteration 95, loss = 0.05480399\n",
      "Iteration 96, loss = 0.05480143\n",
      "Iteration 97, loss = 0.05480015\n",
      "Iteration 98, loss = 0.05479917\n",
      "Iteration 99, loss = 0.05479752\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 100, loss = 0.05479580\n",
      "Iteration 101, loss = 0.05479509\n",
      "Iteration 102, loss = 0.05479473\n",
      "Iteration 103, loss = 0.05479442\n",
      "Iteration 104, loss = 0.05479420\n",
      "Iteration 105, loss = 0.05479398\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 106, loss = 0.05479361\n",
      "Iteration 107, loss = 0.05479347\n",
      "Iteration 108, loss = 0.05479339\n",
      "Iteration 109, loss = 0.05479334\n",
      "Iteration 110, loss = 0.05479328\n",
      "Iteration 111, loss = 0.05479323\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 112, loss = 0.05479317\n",
      "Iteration 113, loss = 0.05479315\n",
      "Iteration 114, loss = 0.05479313\n",
      "Iteration 115, loss = 0.05479312\n",
      "Iteration 116, loss = 0.05479311\n",
      "Iteration 117, loss = 0.05479310\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 118, loss = 0.05479309\n",
      "Iteration 119, loss = 0.05479308\n",
      "Iteration 120, loss = 0.05479308\n",
      "Iteration 121, loss = 0.05479308\n",
      "Iteration 122, loss = 0.05479308\n",
      "Iteration 123, loss = 0.05479307\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp_clf_for_train_loss = MLPClassifier(random_state=42, hidden_layer_sizes=(10, ), alpha=0.001, solver='sgd',\n",
    "                      max_iter=600, n_iter_no_change = 5, learning_rate='adaptive',learning_rate_init = 0.1, activation='logistic', tol = 0.001,\n",
    "                    verbose=10, early_stopping = False)\n",
    "mlp_clf_for_train_loss.fit(X_train, y_train)\n",
    "train_curve_loss = mlp_clf_for_train_loss.loss_curve_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I created the functions below to make the list for the Print out's for ETA, since the classifier doesn't actually save these, and the print out doesn't print out the ETA every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETA_Creation (validation_score, ETA, n_iter_change):\n",
    "    # Takes validation score list, the initial learning rate (ETA), the number of iterations before stop\n",
    "    # It than looks at the number of consecutive changes, and then changes the learning rate by dividing by itself.\n",
    "    a = 0 \n",
    "    list_of_ETA = []\n",
    "    for i in range (len(validation_score)):\n",
    "        #print(i)\n",
    "        if i == (len(validation_score)-1):\n",
    "            #print(\"Ending\")\n",
    "            list_of_ETA.append(\"Early stopping because the validation score change is less than 0.001 over the last 10 epochs.\")\n",
    "        else:\n",
    "            if ((validation_score[i] - validation_score[i-1]) <= 0.001) or (np.abs(validation_score[i] - validation_score[i+1])<= 0.001):\n",
    "                #print(\"similar\",i, validation_score[i], ETA)\n",
    "                a += 1\n",
    "                if a == n_iter_change:\n",
    "                    ETA = ETA / 5\n",
    "                    list_of_ETA.append(ETA)\n",
    "                    a = 0\n",
    "                else:\n",
    "                    list_of_ETA.append(ETA)\n",
    "            else:\n",
    "                #print(\"not similar\",i, validation_score[i],ETA)\n",
    "                a = 0\n",
    "                list_of_ETA.append(ETA)\n",
    "    return list_of_ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded(X, decimal):\n",
    "    if type(X) == \"array\":\n",
    "        round_value = np.around(X, decimal)\n",
    "        X_rounded = list(round_value)\n",
    "    else:\n",
    "        X_array = np.array(X)\n",
    "        rounded_value = np.around(X_array, decimal)\n",
    "        X_rounded = list(rounded_value)\n",
    "        \n",
    "    return X_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ETA = ETA_Creation(validation_scores, 0.1, 10)\n",
    "\n",
    "validation_loss_rounded = rounded(validation_loss, 8)\n",
    "validation_score_rounded = rounded(validation_scores, 8)\n",
    "train_loss_rounded = rounded(train_curve_loss, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I was unable to access the training data from the MLP Classifier that the SKlearn uses. The reason for this is that in the SKlearn documentation, when using the early stop, doesn't save the training loss score. Early stop only looks at the validation score and if there is enough change based upon the tol. But with Early stop turned off instead of looking at the validation score it's looking at the training instead, but it doesn't stop. Because of this the training loss value wouldn't change based upon if early stop is on or off. Becasue of this i utilize this assumption in order to get the trianing loss value. That was what i was attempting to do with the above print out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 120 |  Training Loss: 2.31680675 |  Validation Loss 2.32677393 |  Validation Score: 0.17361111 ETA: 0.1\n",
      "Epoch 1 out of 120 |  Training Loss: 2.11073239 |  Validation Loss 2.14706435 |  Validation Score: 0.53472222 ETA: 0.1\n",
      "Epoch 2 out of 120 |  Training Loss: 1.85807125 |  Validation Loss 1.9455984 |  Validation Score: 0.69444444 ETA: 0.1\n",
      "Epoch 3 out of 120 |  Training Loss: 1.55304091 |  Validation Loss 1.68449718 |  Validation Score: 0.79166667 ETA: 0.1\n",
      "Epoch 4 out of 120 |  Training Loss: 1.27046769 |  Validation Loss 1.42182684 |  Validation Score: 0.83333333 ETA: 0.1\n",
      "Epoch 5 out of 120 |  Training Loss: 1.04303337 |  Validation Loss 1.19175239 |  Validation Score: 0.84722222 ETA: 0.1\n",
      "Epoch 6 out of 120 |  Training Loss: 0.86724393 |  Validation Loss 1.00276736 |  Validation Score: 0.88194444 ETA: 0.1\n",
      "Epoch 7 out of 120 |  Training Loss: 0.72960734 |  Validation Loss 0.85476246 |  Validation Score: 0.90277778 ETA: 0.1\n",
      "Epoch 8 out of 120 |  Training Loss: 0.61883085 |  Validation Loss 0.73406049 |  Validation Score: 0.92361111 ETA: 0.1\n",
      "Epoch 9 out of 120 |  Training Loss: 0.5333584 |  Validation Loss 0.63650775 |  Validation Score: 0.94444444 ETA: 0.1\n",
      "Epoch 10 out of 120 |  Training Loss: 0.46431562 |  Validation Loss 0.5551453 |  Validation Score: 0.95833333 ETA: 0.1\n",
      "Epoch 11 out of 120 |  Training Loss: 0.40877098 |  Validation Loss 0.48990015 |  Validation Score: 0.95833333 ETA: 0.1\n",
      "Epoch 12 out of 120 |  Training Loss: 0.36551199 |  Validation Loss 0.43657108 |  Validation Score: 0.96527778 ETA: 0.1\n",
      "Epoch 13 out of 120 |  Training Loss: 0.32995993 |  Validation Loss 0.39259567 |  Validation Score: 0.97916667 ETA: 0.1\n",
      "Epoch 14 out of 120 |  Training Loss: 0.30161662 |  Validation Loss 0.35730188 |  Validation Score: 0.97916667 ETA: 0.1\n",
      "Epoch 15 out of 120 |  Training Loss: 0.2780045 |  Validation Loss 0.32756785 |  Validation Score: 0.97916667 ETA: 0.1\n",
      "Epoch 16 out of 120 |  Training Loss: 0.25751772 |  Validation Loss 0.3026498 |  Validation Score: 0.97916667 ETA: 0.1\n",
      "Epoch 17 out of 120 |  Training Loss: 0.24108035 |  Validation Loss 0.28125727 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 18 out of 120 |  Training Loss: 0.22673658 |  Validation Loss 0.2635011 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 19 out of 120 |  Training Loss: 0.21412046 |  Validation Loss 0.24753891 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 20 out of 120 |  Training Loss: 0.20293742 |  Validation Loss 0.23361628 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 21 out of 120 |  Training Loss: 0.19308349 |  Validation Loss 0.22131715 |  Validation Score: 0.99305556 ETA: 0.1\n",
      "Epoch 22 out of 120 |  Training Loss: 0.18426923 |  Validation Loss 0.21056192 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 23 out of 120 |  Training Loss: 0.17656102 |  Validation Loss 0.20070928 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 24 out of 120 |  Training Loss: 0.16912276 |  Validation Loss 0.1921605 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 25 out of 120 |  Training Loss: 0.1626882 |  Validation Loss 0.18459266 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 26 out of 120 |  Training Loss: 0.15678801 |  Validation Loss 0.1768561 |  Validation Score: 0.97916667 ETA: 0.1\n",
      "Epoch 27 out of 120 |  Training Loss: 0.15088539 |  Validation Loss 0.17056956 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 28 out of 120 |  Training Loss: 0.14587784 |  Validation Loss 0.16445154 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 29 out of 120 |  Training Loss: 0.14101754 |  Validation Loss 0.15892238 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 30 out of 120 |  Training Loss: 0.13672817 |  Validation Loss 0.15382091 |  Validation Score: 0.98611111 ETA: 0.1\n",
      "Epoch 31 out of 120 |  Training Loss: 0.13278614 |  Validation Loss 0.14880128 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 32 out of 120 |  Training Loss: 0.12889304 |  Validation Loss 0.14479297 |  Validation Score: 0.97916667 ETA: 0.02\n",
      "Epoch 33 out of 120 |  Training Loss: 0.12505022 |  Validation Loss 0.14012744 |  Validation Score: 0.97916667 ETA: 0.02\n",
      "Epoch 34 out of 120 |  Training Loss: 0.12159634 |  Validation Loss 0.13778212 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 35 out of 120 |  Training Loss: 0.1184714 |  Validation Loss 0.13627291 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 36 out of 120 |  Training Loss: 0.11556273 |  Validation Loss 0.13518908 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 37 out of 120 |  Training Loss: 0.11266975 |  Validation Loss 0.13426306 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 38 out of 120 |  Training Loss: 0.11001495 |  Validation Loss 0.1334203 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 39 out of 120 |  Training Loss: 0.1074041 |  Validation Loss 0.13266563 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 40 out of 120 |  Training Loss: 0.10494645 |  Validation Loss 0.13194267 |  Validation Score: 0.98611111 ETA: 0.02\n",
      "Epoch 41 out of 120 |  Training Loss: 0.10277112 |  Validation Loss 0.13125415 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 42 out of 120 |  Training Loss: 0.10043252 |  Validation Loss 0.13056539 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 43 out of 120 |  Training Loss: 0.09808229 |  Validation Loss 0.12985547 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 44 out of 120 |  Training Loss: 0.0958811 |  Validation Loss 0.12915965 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 45 out of 120 |  Training Loss: 0.09394964 |  Validation Loss 0.1287846 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 46 out of 120 |  Training Loss: 0.09201512 |  Validation Loss 0.12851224 |  Validation Score: 0.97916667 ETA: 0.004\n",
      "Epoch 47 out of 120 |  Training Loss: 0.08982671 |  Validation Loss 0.12833166 |  Validation Score: 0.97916667 ETA: 0.004\n",
      "Epoch 48 out of 120 |  Training Loss: 0.088306 |  Validation Loss 0.12817465 |  Validation Score: 0.97916667 ETA: 0.004\n",
      "Epoch 49 out of 120 |  Training Loss: 0.08653554 |  Validation Loss 0.12803121 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 50 out of 120 |  Training Loss: 0.08438693 |  Validation Loss 0.127892 |  Validation Score: 0.98611111 ETA: 0.004\n",
      "Epoch 51 out of 120 |  Training Loss: 0.0829147 |  Validation Loss 0.12775847 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 52 out of 120 |  Training Loss: 0.08113748 |  Validation Loss 0.12762472 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 53 out of 120 |  Training Loss: 0.07952699 |  Validation Loss 0.12750128 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 54 out of 120 |  Training Loss: 0.07770096 |  Validation Loss 0.12736659 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 55 out of 120 |  Training Loss: 0.0762459 |  Validation Loss 0.12722771 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 56 out of 120 |  Training Loss: 0.07457217 |  Validation Loss 0.12715753 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 57 out of 120 |  Training Loss: 0.07349928 |  Validation Loss 0.12711026 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 58 out of 120 |  Training Loss: 0.07225354 |  Validation Loss 0.12707169 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 59 out of 120 |  Training Loss: 0.0710749 |  Validation Loss 0.1270414 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 60 out of 120 |  Training Loss: 0.06992542 |  Validation Loss 0.12701266 |  Validation Score: 0.98611111 ETA: 0.0008\n",
      "Epoch 61 out of 120 |  Training Loss: 0.06855146 |  Validation Loss 0.12698656 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 62 out of 120 |  Training Loss: 0.06759786 |  Validation Loss 0.12695879 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 63 out of 120 |  Training Loss: 0.06644576 |  Validation Loss 0.1269354 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 64 out of 120 |  Training Loss: 0.06572877 |  Validation Loss 0.12690823 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 65 out of 120 |  Training Loss: 0.06482791 |  Validation Loss 0.12688206 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 66 out of 120 |  Training Loss: 0.06353192 |  Validation Loss 0.12685487 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 67 out of 120 |  Training Loss: 0.06243969 |  Validation Loss 0.12684081 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 68 out of 120 |  Training Loss: 0.06158304 |  Validation Loss 0.12683105 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 69 out of 120 |  Training Loss: 0.0607953 |  Validation Loss 0.12682384 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 70 out of 120 |  Training Loss: 0.0599497 |  Validation Loss 0.12681792 |  Validation Score: 0.98611111 ETA: 0.00016\n",
      "Epoch 71 out of 120 |  Training Loss: 0.05911512 |  Validation Loss 0.12681236 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 72 out of 120 |  Training Loss: 0.05829429 |  Validation Loss 0.12680666 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 73 out of 120 |  Training Loss: 0.05764038 |  Validation Loss 0.12680179 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 74 out of 120 |  Training Loss: 0.05663065 |  Validation Loss 0.12679633 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 75 out of 120 |  Training Loss: 0.05622247 |  Validation Loss 0.12679144 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 76 out of 120 |  Training Loss: 0.05596339 |  Validation Loss 0.12678624 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 77 out of 120 |  Training Loss: 0.05575154 |  Validation Loss 0.12678075 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 78 out of 120 |  Training Loss: 0.05559862 |  Validation Loss 0.12677785 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 79 out of 120 |  Training Loss: 0.05542939 |  Validation Loss 0.12677586 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 80 out of 120 |  Training Loss: 0.05529633 |  Validation Loss 0.12677447 |  Validation Score: 0.98611111 ETA: 3.2000000000000005e-05\n",
      "Epoch 81 out of 120 |  Training Loss: 0.05513348 |  Validation Loss 0.12677319 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 82 out of 120 |  Training Loss: 0.05503971 |  Validation Loss 0.12677207 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 83 out of 120 |  Training Loss: 0.05499573 |  Validation Loss 0.12677102 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 84 out of 120 |  Training Loss: 0.05495802 |  Validation Loss 0.12676997 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 85 out of 120 |  Training Loss: 0.05492411 |  Validation Loss 0.12676895 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 86 out of 120 |  Training Loss: 0.05489663 |  Validation Loss 0.1267679 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 87 out of 120 |  Training Loss: 0.05486048 |  Validation Loss 0.12676683 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 88 out of 120 |  Training Loss: 0.05484523 |  Validation Loss 0.12676581 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 89 out of 120 |  Training Loss: 0.05483602 |  Validation Loss 0.12676519 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 90 out of 120 |  Training Loss: 0.05482883 |  Validation Loss 0.12676481 |  Validation Score: 0.98611111 ETA: 6.400000000000001e-06\n",
      "Epoch 91 out of 120 |  Training Loss: 0.05482205 |  Validation Loss 0.12676451 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 92 out of 120 |  Training Loss: 0.05481529 |  Validation Loss 0.12676428 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 93 out of 120 |  Training Loss: 0.05480779 |  Validation Loss 0.12676405 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 94 out of 120 |  Training Loss: 0.05480399 |  Validation Loss 0.12676383 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 95 out of 120 |  Training Loss: 0.05480143 |  Validation Loss 0.12676363 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 96 out of 120 |  Training Loss: 0.05480015 |  Validation Loss 0.12676342 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 97 out of 120 |  Training Loss: 0.05479917 |  Validation Loss 0.12676321 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 98 out of 120 |  Training Loss: 0.05479752 |  Validation Loss 0.12676301 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 99 out of 120 |  Training Loss: 0.0547958 |  Validation Loss 0.12676279 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 100 out of 120 |  Training Loss: 0.05479509 |  Validation Loss 0.12676267 |  Validation Score: 0.98611111 ETA: 1.2800000000000002e-06\n",
      "Epoch 101 out of 120 |  Training Loss: 0.05479473 |  Validation Loss 0.1267626 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 102 out of 120 |  Training Loss: 0.05479442 |  Validation Loss 0.12676254 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 103 out of 120 |  Training Loss: 0.0547942 |  Validation Loss 0.12676249 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 104 out of 120 |  Training Loss: 0.05479398 |  Validation Loss 0.12676244 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 105 out of 120 |  Training Loss: 0.05479361 |  Validation Loss 0.1267624 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 106 out of 120 |  Training Loss: 0.05479347 |  Validation Loss 0.12676236 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 107 out of 120 |  Training Loss: 0.05479339 |  Validation Loss 0.12676232 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 108 out of 120 |  Training Loss: 0.05479334 |  Validation Loss 0.12676228 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 109 out of 120 |  Training Loss: 0.05479328 |  Validation Loss 0.12676224 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 110 out of 120 |  Training Loss: 0.05479323 |  Validation Loss 0.12676219 |  Validation Score: 0.98611111 ETA: 2.5600000000000007e-07\n",
      "Epoch 111 out of 120 |  Training Loss: 0.05479317 |  Validation Loss 0.12676217 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 112 out of 120 |  Training Loss: 0.05479315 |  Validation Loss 0.12676215 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 113 out of 120 |  Training Loss: 0.05479313 |  Validation Loss 0.12676214 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 114 out of 120 |  Training Loss: 0.05479312 |  Validation Loss 0.12676213 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 115 out of 120 |  Training Loss: 0.05479311 |  Validation Loss 0.12676212 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 116 out of 120 |  Training Loss: 0.0547931 |  Validation Loss 0.12676211 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 117 out of 120 |  Training Loss: 0.05479309 |  Validation Loss 0.1267621 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 118 out of 120 |  Training Loss: 0.05479308 |  Validation Loss 0.1267621 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 119 out of 120 |  Training Loss: 0.05479308 |  Validation Loss 0.12676209 |  Validation Score: 0.98611111 ETA: 5.1200000000000015e-08\n",
      "Epoch 120 out of 120 |  Early stopping because the validation score change is less than 0.001 over the last 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(validation_loss)):\n",
    "    if i == (len(validation_loss)-1):\n",
    "        print(\"Epoch\", i,\"out of\", (len(validation_loss)-1),\"| \", list_of_ETA[i])\n",
    "    else:\n",
    "        print(\"Epoch\", i,\"out of\", (len(validation_loss)-1), \"|  Training Loss:\", train_loss_rounded[i], \"|  Validation Loss\", validation_loss_rounded[i], \"|  Validation Score:\", validation_score_rounded[i], \"ETA:\", list_of_ETA[i] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Validation Score')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp_clf.validation_scores_, '-b', label = \"Validation Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Score vs Epoch\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp_clf.loss_curve_, '-b', label = \"Validation Loss\")\n",
    "plt.plot(mlp_clf_for_train_loss.loss_curve_, '-r', label = \"Training Loss\")\n",
    "plt.title(\"Training And Validation Loss\") \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Score\")\n",
    "plt.legend()\n",
    "validation_loss = mlp_clf.loss_curve_\n",
    "validation_score = mlp_clf.validation_scores_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "\n",
      "Training Accuracy:  0.9770354906054279\n",
      "\n",
      "Train - No. Of Correct Predictions 1404.0 / 1437\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[143   0   0   0   1   0   1   0   0   0]\n",
      " [  0 150   0   0   1   0   0   0   2   1]\n",
      " [  0   2 141   1   0   0   0   0   0   0]\n",
      " [  0   0   0 148   0   0   0   0   1   0]\n",
      " [  0   1   0   0 130   0   0   2   0   2]\n",
      " [  0   0   0   0   1 132   0   0   0   2]\n",
      " [  0   1   0   0   0   0 145   0   0   0]\n",
      " [  0   0   0   0   0   0   0 145   0   0]\n",
      " [  0   5   0   0   0   1   2   0 136   0]\n",
      " [  0   0   0   1   0   2   0   1   2 134]]\n",
      "\n",
      "Test Precision = 0.977182\n",
      "Test Recall = 0.977035\n",
      "Test F1 Score = 0.977033\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       145\n",
      "           1       0.94      0.97      0.96       154\n",
      "           2       1.00      0.98      0.99       144\n",
      "           3       0.99      0.99      0.99       149\n",
      "           4       0.98      0.96      0.97       135\n",
      "           5       0.98      0.98      0.98       135\n",
      "           6       0.98      0.99      0.99       146\n",
      "           7       0.98      1.00      0.99       145\n",
      "           8       0.96      0.94      0.95       144\n",
      "           9       0.96      0.96      0.96       140\n",
      "\n",
      "    accuracy                           0.98      1437\n",
      "   macro avg       0.98      0.98      0.98      1437\n",
      "weighted avg       0.98      0.98      0.98      1437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data\")\n",
    "\n",
    "y_train_predicted = mlp_clf.predict(X_train)\n",
    "\n",
    "train_accuracy_mlp = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_mlp)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "train_no_correct = train_size  * train_accuracy_mlp\n",
    "\n",
    "print (\"\\nTrain - No. Of Correct Predictions\", train_no_correct, \"/\", train_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_train, y_train_predicted,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_train, y_train_predicted,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_train, y_train_predicted,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train, y_train_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data\n",
      "\n",
      "Test Accuracy:  0.9638888888888889\n",
      "\n",
      "Test - No. Of Correct Predictions 347.0 / 360\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[32  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 27  1  0  0  0  0  0  0  0]\n",
      " [ 0  0 33  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 32  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 46  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 45  1  0  0  1]\n",
      " [ 0  0  0  0  0  1 34  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 33  0  0]\n",
      " [ 0  2  0  0  0  1  0  0 27  0]\n",
      " [ 0  0  0  1  0  1  0  0  0 38]]\n",
      "\n",
      "Test Precision = 0.965135\n",
      "Test Recall = 0.963889\n",
      "Test F1 Score = 0.963983\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        33\n",
      "           1       0.93      0.96      0.95        28\n",
      "           2       0.94      1.00      0.97        33\n",
      "           3       0.97      0.94      0.96        34\n",
      "           4       0.98      1.00      0.99        46\n",
      "           5       0.90      0.96      0.93        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       1.00      0.90      0.95        30\n",
      "           9       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.97      0.96      0.96       360\n",
      "weighted avg       0.97      0.96      0.96       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Data\")\n",
    "\n",
    "y_test_predicted = mlp_clf.predict(X_test)\n",
    "\n",
    "test_accuracy_mlp = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy_mlp)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy_mlp\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
